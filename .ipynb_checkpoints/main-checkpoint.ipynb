{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e03cb0f8-1376-438a-87d6-0f0018524bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 16:53:17.439922: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-09 16:53:17.465384: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-09 16:53:17.680282: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-09 16:53:17.681812: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-09 16:53:18.235024: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import LSTM, RepeatVector, TimeDistributed, Dense\n",
    "from keras.models import Sequential\n",
    "from keras import Input\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "import collections\n",
    "from random import shuffle\n",
    "import itertools\n",
    "from os import listdir\n",
    "import random\n",
    "import string\n",
    "import statistics\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6033c331-d70b-4fe4-b69e-03603fa31c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.13.1\n",
      "NumPy: 1.23.5\n"
     ]
    }
   ],
   "source": [
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d9a31c1-ff4e-4132-a307-d78d2100c050",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'Weight on Bit (klbs)',\n",
    "    'Rotary RPM (RPM)',\n",
    "    'Total Pump Output (gal_per_min)',\n",
    "    'Rate Of Penetration (ft_per_hr)',\n",
    "    'Standpipe Pressure (psi)',\n",
    "    'Rotary Torque (kft_lb)', \n",
    "    'Hole Depth (feet)', \n",
    "    'Bit Depth (feet)'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a72daff5-bb2f-4f29-b5d9-f657e789d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_windows(dataset, columns):\n",
    "    df = pd.read_csv(os.path.join(\"Datasets\", \"MaskedAutoencoder\", dataset))\n",
    "    df = df[columns]\n",
    "\n",
    "    base_mask = (\n",
    "        (df[\"Hole Depth (feet)\"].rolling(10000).mean().diff() > 0) &\n",
    "        (df[\"Hole Depth (feet)\"] == df[\"Bit Depth (feet)\"]) &\n",
    "        (df[\"Hole Depth (feet)\"] > 1000)\n",
    "    )\n",
    "    \n",
    "    window = 100       # Rolling window size\n",
    "    threshold = 0.3    # Keep if rolling average > threshold\n",
    "    \n",
    "    # Compute rolling average of the mask (convert to 0/1 first)\n",
    "    rolling_avg = base_mask.astype(float).rolling(window).mean()\n",
    "    \n",
    "    # Final mask based on rolling average threshold\n",
    "    final_mask = (rolling_avg > threshold).fillna(0)\n",
    "    \n",
    "    final_mask = final_mask.astype(float).rolling(20000).mean() > 0.6\n",
    "    \n",
    "    masked_hole_depth = df[\"Hole Depth (feet)\"].where(final_mask, np.nan)\n",
    "    \n",
    "    gap_threshold = 100  # maximum number of consecutive NaNs to merge segments\n",
    "    \n",
    "    # Identify indices of non-NaN values\n",
    "    not_nan_idx = masked_hole_depth[masked_hole_depth.notna()].index\n",
    "    \n",
    "    # Grouping non-NaN indices based on closeness\n",
    "    groups = []\n",
    "    current_group = []\n",
    "    \n",
    "    for i, idx in enumerate(not_nan_idx):\n",
    "        if i == 0:\n",
    "            current_group.append(idx)\n",
    "            continue\n",
    "    \n",
    "        # Check gap from previous index\n",
    "        if idx - not_nan_idx[i-1] <= gap_threshold:\n",
    "            current_group.append(idx)\n",
    "        else:\n",
    "            groups.append(current_group)\n",
    "            current_group = [idx]\n",
    "    \n",
    "    # Append last group\n",
    "    if current_group:\n",
    "        groups.append(current_group)\n",
    "\n",
    "    # Fix all NaNs\n",
    "    drilling_segments = [  ]\n",
    "    window_size = 100\n",
    "    for group in groups:\n",
    "        dfg = df.loc[group].copy()\n",
    "        \n",
    "        for col in dfg.columns:\n",
    "            if np.issubdtype(dfg[col].dtype, np.number):\n",
    "                series = dfg[col]      \n",
    "                rolling_mean = series.rolling(window=window_size, min_periods=1, center=True).mean()\n",
    "                dfg[col] = series.fillna(rolling_mean).bfill(  ).ffill()\n",
    "    \n",
    "        drilling_segments.append(dfg)\n",
    "    \n",
    "    # Min Max Normalization\n",
    "    global_min = pd.concat(drilling_segments).min()\n",
    "    global_max = pd.concat(drilling_segments).max()\n",
    "    \n",
    "    # Step 2: Normalize each dataframe\n",
    "    print(f\"Drilling Segments: {len(drilling_segments)}\")\n",
    "    normalized_drilling_segments = []\n",
    "    for df in drilling_segments:\n",
    "        normalized_df = (df - global_min) / (global_max - global_min)\n",
    "        normalized_drilling_segments.append(normalized_df)\n",
    "\n",
    "    window_size = 60 * 10  # 10 minutes\n",
    "\n",
    "    windows = []\n",
    "    count = 1\n",
    "    for df in normalized_drilling_segments:\n",
    "        print(f\"\\t{count}\")\n",
    "        count += 1\n",
    "        for i in range(len(df) - window_size + 1):\n",
    "            window = df.iloc[i:i + window_size]\n",
    "            windows.append(window.to_numpy())\n",
    "\n",
    "    print(f\"Windows: {len(windows):,}\".replace(',', ' ')) \n",
    "    print(f\"Windows per Segment: {len(windows) / len(drilling_segments):,.2f}\".replace(',', ' '))\n",
    "    \n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81ed19b3-8fee-4b2a-bf9d-93925474f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_data(data, MASKING_PERCENT=0.8):\n",
    "    masked_data = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"Processing: {i}/{len(data)} ({100*i/len(data):.1f}%)\")\n",
    "    \n",
    "        arr = np.array(data[i], dtype=np.float32)  # Use float32 instead of float64\n",
    "        masked_arr = arr.copy()\n",
    "        \n",
    "        total_elements = arr.size\n",
    "        n_mask = int(total_elements * MASKING_PERCENT)\n",
    "        \n",
    "        flat_indices = np.random.choice(total_elements, size=n_mask, replace=False)\n",
    "        mask_indices = np.unravel_index(flat_indices, arr.shape)\n",
    "        \n",
    "        masked_arr[mask_indices] = 0  # Use 0 instead of NaN for neural networks\n",
    "        \n",
    "        masked_data.append(masked_arr)\n",
    "    \n",
    "    return np.array(masked_data, dtype=np.float32)  # Return as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0bff3db-3c83-4e70-9d34-93d94d93bd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data, batch_size=32, mask_percent=0.8, shuffle=True):\n",
    "        self.data = np.array(data, dtype=np.float32)\n",
    "        self.batch_size = batch_size\n",
    "        self.mask_percent = mask_percent\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(data))\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.data[batch_indices]\n",
    "        \n",
    "        # Create masked version (X) from original (Y)\n",
    "        batch_x = batch_y.copy()\n",
    "        for i in range(len(batch_x)):\n",
    "            n_mask = int(batch_x[i].size * self.mask_percent)\n",
    "            flat_indices = np.random.choice(batch_x[i].size, size=n_mask, replace=False)\n",
    "            mask_indices = np.unravel_index(flat_indices, batch_x[i].shape)\n",
    "            batch_x[i][mask_indices] = 0\n",
    "            \n",
    "        return batch_x, batch_y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6182e07c-0156-4180-b2c3-70dbbc8ca326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder training: 78B-32 1 sec data 27200701.csv, 27029986-3.csv\n",
    "# Task Header 1 (DAS Stickslip): 27029986-4.csv\n",
    "# Task Header 2 (Temp OUT (Degrees)): 27029986-5.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8b7142e-8ece-47a0-95ab-311c27f332c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drilling Segments: 3\n",
      "\t1\n",
      "\t2\n",
      "\t3\n",
      "Windows: 131 072\n",
      "Windows per Segment: 43 690.67\n",
      "Drilling Segments: 9\n",
      "\t1\n",
      "\t2\n",
      "\t3\n",
      "\t4\n",
      "\t5\n",
      "\t6\n",
      "\t7\n",
      "\t8\n",
      "\t9\n",
      "Windows: 332 561\n",
      "Windows per Segment: 36 951.22\n",
      "Sampled 131 072 from each list\n",
      "Total windows: 262 144\n"
     ]
    }
   ],
   "source": [
    "windows1 = csv_to_windows(\"27029986-3.csv\", columns)\n",
    "windows2 = csv_to_windows(\"78B-32 1 sec data 27200701.csv\", columns)\n",
    "\n",
    "# Shuffle both lists\n",
    "random.seed(42)\n",
    "random.shuffle(windows1)\n",
    "random.shuffle(windows2)\n",
    "\n",
    "# Take the same amount from each (the minimum length)\n",
    "min_length = min(len(windows1), len(windows2))\n",
    "windows1_sampled = windows1[:min_length]\n",
    "windows2_sampled = windows2[:min_length]\n",
    "\n",
    "# Combine them\n",
    "windows = windows1_sampled + windows2_sampled\n",
    "\n",
    "# Shuffle the combined list\n",
    "random.shuffle(windows)\n",
    "\n",
    "print(f\"Sampled {min_length:,} from each list\".replace(',', ' '))\n",
    "print(f\"Total windows: {len(windows):,}\".replace(',', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77fa2d00-47e3-46a3-ad90-b3ee040123dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (209715, 600, 8)\n",
      "Test shape: (52429, 600, 8)\n",
      "Memory for train_y: 4.03 GB\n",
      "Memory for test_y: 1.01 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 - Split into train/test and convert to numpy ONCE\n",
    "train_windows, test_windows = train_test_split(windows, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to numpy arrays (only store Y, not X)\n",
    "train_windows_y = np.array(train_windows, dtype=np.float32)\n",
    "test_windows_y = np.array(test_windows, dtype=np.float32)\n",
    "\n",
    "# Free up memory\n",
    "del windows, windows1, windows2, windows1_sampled, windows2_sampled, train_windows, test_windows\n",
    "\n",
    "print(f\"Train shape: {train_windows_y.shape}\")\n",
    "print(f\"Test shape: {test_windows_y.shape}\")\n",
    "print(f\"Memory for train_y: {train_windows_y.nbytes / 1e9:.2f} GB\")\n",
    "print(f\"Memory for test_y: {test_windows_y.nbytes / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce5ede37-182a-43a4-8340-982433b5af2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches per epoch: 6554\n",
      "Test batches: 1639\n",
      "Batch X shape: (32, 600, 8)\n",
      "Batch Y shape: (32, 600, 8)\n",
      "Masking working: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 - Create generators (NO masking done here, just setup)\n",
    "train_gen = MaskedDataGenerator(train_windows_y, batch_size=32, mask_percent=0.8, shuffle=True)\n",
    "test_gen = MaskedDataGenerator(test_windows_y, batch_size=32, mask_percent=0.8, shuffle=False)\n",
    "\n",
    "print(f\"Train batches per epoch: {len(train_gen)}\")\n",
    "print(f\"Test batches: {len(test_gen)}\")\n",
    "\n",
    "# Test that it works\n",
    "batch_x, batch_y = train_gen[0]\n",
    "print(f\"Batch X shape: {batch_x.shape}\")\n",
    "print(f\"Batch Y shape: {batch_y.shape}\")\n",
    "print(f\"Masking working: {np.sum(batch_x == 0) > 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c45051a5-5e72-48c1-ac64-9e024e067f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Memory-efficient callback created!\n",
      "   Will evaluate ~655 training batches and ~163 test batches per epoch\n"
     ]
    }
   ],
   "source": [
    "# Replace your MetricsCallback class (Cell 11) with this improved version:\n",
    "\n",
    "class MetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, train_gen, test_gen, train_y, test_y, subset_fraction=0.1):\n",
    "        super().__init__()\n",
    "        self.train_gen = train_gen\n",
    "        self.test_gen = test_gen\n",
    "        self.subset_fraction = subset_fraction\n",
    "        self.history = {\n",
    "            'train_rmse': [],\n",
    "            'train_mae': [],\n",
    "            'test_rmse': [],\n",
    "            'test_mae': []\n",
    "        }\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        import gc\n",
    "        import tensorflow.keras.backend as K\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Calculating metrics for epoch {epoch + 1}...\")\n",
    "        print(f\"   (Evaluating on {self.subset_fraction*100:.0f}% of batches)\")\n",
    "        \n",
    "        max_train_batches = max(1, int(len(self.train_gen) * self.subset_fraction))\n",
    "        max_test_batches = max(1, int(len(self.test_gen) * self.subset_fraction))\n",
    "        \n",
    "        # === TRAINING METRICS ===\n",
    "        train_mse_sum = 0\n",
    "        train_mae_sum = 0\n",
    "        train_samples = 0\n",
    "        \n",
    "        print(f\"   Evaluating training set ({max_train_batches} batches)...\", end=\"\", flush=True)\n",
    "        for i in range(max_train_batches):\n",
    "            batch_x, batch_y = self.train_gen[i]\n",
    "            \n",
    "            # Use predict_on_batch to avoid memory accumulation\n",
    "            batch_pred = self.model.predict_on_batch(batch_x)\n",
    "            \n",
    "            # Calculate metrics immediately\n",
    "            batch_mse = np.mean((batch_y - batch_pred) ** 2)\n",
    "            batch_mae = np.mean(np.abs(batch_y - batch_pred))\n",
    "            batch_size = len(batch_y)\n",
    "            \n",
    "            train_mse_sum += batch_mse * batch_size\n",
    "            train_mae_sum += batch_mae * batch_size\n",
    "            train_samples += batch_size\n",
    "            \n",
    "            # Clear variables immediately\n",
    "            del batch_x, batch_y, batch_pred\n",
    "            \n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"\\r   Evaluating training set... {i+1}/{max_train_batches}\", end=\"\", flush=True)\n",
    "                gc.collect()\n",
    "                K.clear_session()  # Clear TensorFlow session\n",
    "        \n",
    "        train_rmse = np.sqrt(train_mse_sum / train_samples)\n",
    "        train_mae = train_mae_sum / train_samples\n",
    "        print(f\"\\r   Training set complete! ({train_samples} samples)         \")\n",
    "        \n",
    "        # === TEST METRICS ===\n",
    "        test_mse_sum = 0\n",
    "        test_mae_sum = 0\n",
    "        test_samples = 0\n",
    "        \n",
    "        print(f\"   Evaluating test set ({max_test_batches} batches)...\", end=\"\", flush=True)\n",
    "        for i in range(max_test_batches):\n",
    "            batch_x, batch_y = self.test_gen[i]\n",
    "            \n",
    "            # Use predict_on_batch\n",
    "            batch_pred = self.model.predict_on_batch(batch_x)\n",
    "            \n",
    "            batch_mse = np.mean((batch_y - batch_pred) ** 2)\n",
    "            batch_mae = np.mean(np.abs(batch_y - batch_pred))\n",
    "            batch_size = len(batch_y)\n",
    "            \n",
    "            test_mse_sum += batch_mse * batch_size\n",
    "            test_mae_sum += batch_mae * batch_size\n",
    "            test_samples += batch_size\n",
    "            \n",
    "            # Clear immediately\n",
    "            del batch_x, batch_y, batch_pred\n",
    "            \n",
    "            if (i + 1) % 25 == 0:\n",
    "                print(f\"\\r   Evaluating test set... {i+1}/{max_test_batches}\", end=\"\", flush=True)\n",
    "                gc.collect()\n",
    "                K.clear_session()\n",
    "        \n",
    "        test_rmse = np.sqrt(test_mse_sum / test_samples)\n",
    "        test_mae = test_mae_sum / test_samples\n",
    "        print(f\"\\r   Test set complete! ({test_samples} samples)         \")\n",
    "        \n",
    "        # Store metrics\n",
    "        self.history['train_rmse'].append(float(train_rmse))\n",
    "        self.history['train_mae'].append(float(train_mae))\n",
    "        self.history['test_rmse'].append(float(test_rmse))\n",
    "        self.history['test_mae'].append(float(test_mae))\n",
    "        \n",
    "        print(f\"   Train - RMSE: {train_rmse:.6f}, MAE: {train_mae:.6f}\")\n",
    "        print(f\"   Test  - RMSE: {test_rmse:.6f}, MAE: {test_mae:.6f}\")\n",
    "        \n",
    "        # Final cleanup\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "\n",
    "\n",
    "\n",
    "# In Cell 11 where you create metrics_callback, reduce from 0.1 to 0.05:\n",
    "metrics_callback = MetricsCallback(\n",
    "    train_gen, \n",
    "    test_gen, \n",
    "    train_windows_y, \n",
    "    test_windows_y,\n",
    "    subset_fraction=0.10  # Evaluate only 5% instead of 10%\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8596d76f-c197-4255-9f29-96c0b21d74e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 16:53:57.310772: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 600, 128)          70144     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " repeat_vector (RepeatVecto  (None, 600, 64)           0         \n",
      " r)                                                              \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 600, 64)           33024     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 600, 128)          98816     \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 600, 8)            1032      \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 252424 (986.03 KB)\n",
      "Trainable params: 252424 (986.03 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Cell 10 - Build model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, activation='tanh', input_shape=(600, 8), return_sequences=True))\n",
    "model.add(LSTM(64, activation='tanh', return_sequences=False))\n",
    "model.add(RepeatVector(600))\n",
    "model.add(LSTM(64, activation='tanh', return_sequences=True))\n",
    "model.add(LSTM(128, activation='tanh', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(8)))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1074ecb-8334-482d-acb5-2752242a0b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6554/6554 [==============================] - ETA: 0s - loss: 0.0233\n",
      "ðŸ“Š Calculating metrics for epoch 1...\n",
      "   (Evaluating on 10% of batches)\n",
      "   Training set complete! (20960 samples)         \n",
      "   Test set complete! (5216 samples)         \n",
      "   Train - RMSE: 0.112516, MAE: 0.081813\n",
      "   Test  - RMSE: 0.111590, MAE: 0.081049\n",
      "6554/6554 [==============================] - 2540s 387ms/step - loss: 0.0233 - val_loss: 0.0127\n",
      "Epoch 2/10\n",
      "6554/6554 [==============================] - ETA: 0s - loss: 0.0055\n",
      "ðŸ“Š Calculating metrics for epoch 2...\n",
      "   (Evaluating on 10% of batches)\n",
      "   Training set complete! (20960 samples)         \n",
      "   Test set complete! (5216 samples)         \n",
      "   Train - RMSE: 0.054464, MAE: 0.024656\n",
      "   Test  - RMSE: 0.054419, MAE: 0.024596\n",
      "6554/6554 [==============================] - 2538s 387ms/step - loss: 0.0055 - val_loss: 0.0030\n",
      "Epoch 3/10\n",
      "6554/6554 [==============================] - ETA: 0s - loss: 0.0029\n",
      "ðŸ“Š Calculating metrics for epoch 3...\n",
      "   (Evaluating on 10% of batches)\n",
      "   Training set complete! (20960 samples)         \n",
      "   Test set complete! (5216 samples)         \n",
      "   Train - RMSE: 0.049245, MAE: 0.021282\n",
      "   Test  - RMSE: 0.048748, MAE: 0.021197\n",
      "6554/6554 [==============================] - 2539s 387ms/step - loss: 0.0029 - val_loss: 0.0024\n",
      "Epoch 4/10\n",
      "6554/6554 [==============================] - ETA: 0s - loss: 0.0017\n",
      "ðŸ“Š Calculating metrics for epoch 4...\n",
      "   (Evaluating on 10% of batches)\n",
      "   Training set complete! (20960 samples)         \n",
      "   Test set complete! (5216 samples)         \n",
      "   Train - RMSE: 0.035401, MAE: 0.014865\n",
      "   Test  - RMSE: 0.034698, MAE: 0.014720\n",
      "6554/6554 [==============================] - 2545s 388ms/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 5/10\n",
      "6554/6554 [==============================] - ETA: 0s - loss: 0.0011\n",
      "ðŸ“Š Calculating metrics for epoch 5...\n",
      "   (Evaluating on 10% of batches)\n",
      "   Training set complete! (20960 samples)         \n",
      "   Test set complete! (5216 samples)         \n",
      "   Train - RMSE: 0.030061, MAE: 0.011442\n",
      "   Test  - RMSE: 0.029737, MAE: 0.011344\n",
      "6554/6554 [==============================] - 2535s 387ms/step - loss: 0.0011 - val_loss: 9.2912e-04\n",
      "Epoch 6/10\n",
      "6554/6554 [==============================] - ETA: 0s - loss: 9.6744e-04\n",
      "ðŸ“Š Calculating metrics for epoch 6...\n",
      "   (Evaluating on 10% of batches)\n",
      "   Training set complete! (20960 samples)         \n",
      "   Test set complete! (5216 samples)         \n",
      "   Train - RMSE: 0.029704, MAE: 0.011359\n",
      "   Test  - RMSE: 0.028765, MAE: 0.011112\n",
      "6554/6554 [==============================] - 2536s 387ms/step - loss: 9.6744e-04 - val_loss: 8.7139e-04\n",
      "Epoch 7/10\n",
      "4141/6554 [=================>............] - ETA: 13:44 - loss: 8.4606e-04"
     ]
    }
   ],
   "source": [
    "# Cell 11 - Train model with the callback\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    epochs=10,\n",
    "    validation_data=test_gen,\n",
    "    callbacks=[metrics_callback],  # Add the callback here\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40652bb9-2646-45e5-abcd-866bcf255984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['loss'], label='Training loss', marker='o')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss', marker='o')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training History - Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot RMSE\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(metrics_callback.history['train_rmse'], label='Train RMSE', marker='o')\n",
    "plt.plot(metrics_callback.history['test_rmse'], label='Test RMSE', marker='o')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Training History - RMSE')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot MAE\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(metrics_callback.history['train_mae'], label='Train MAE', marker='o')\n",
    "plt.plot(metrics_callback.history['test_mae'], label='Test MAE', marker='o')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('Training History - MAE')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0701a0-7455-486f-b4c4-ea54bf4d4d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12 - Display final metrics table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"METRICS PER EPOCH\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Epoch':<8} {'Train RMSE':<15} {'Train MAE':<15} {'Test RMSE':<15} {'Test MAE':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i in range(len(metrics_callback.history['train_rmse'])):\n",
    "    print(f\"{i+1:<8} \"\n",
    "          f\"{metrics_callback.history['train_rmse'][i]:<15.6f} \"\n",
    "          f\"{metrics_callback.history['train_mae'][i]:<15.6f} \"\n",
    "          f\"{metrics_callback.history['test_rmse'][i]:<15.6f} \"\n",
    "          f\"{metrics_callback.history['test_mae'][i]:<15.6f}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Final epoch summary\n",
    "final_epoch = len(metrics_callback.history['train_rmse']) - 1\n",
    "print(f\"\\nðŸ“Š Final Epoch ({final_epoch + 1}) Summary:\")\n",
    "print(f\"   Training   - RMSE: {metrics_callback.history['train_rmse'][final_epoch]:.6f}, \"\n",
    "      f\"MAE: {metrics_callback.history['train_mae'][final_epoch]:.6f}\")\n",
    "print(f\"   Test       - RMSE: {metrics_callback.history['test_rmse'][final_epoch]:.6f}, \"\n",
    "      f\"MAE: {metrics_callback.history['test_mae'][final_epoch]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e1b192-9765-4130-9837-646e079bc69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a few random test samples\n",
    "np.random.seed(42)\n",
    "num_examples = 3\n",
    "random_indices = np.random.choice(len(test_windows_y), num_examples, replace=False)\n",
    "\n",
    "# Get the samples\n",
    "sample_y = test_windows_y[random_indices]\n",
    "\n",
    "# Create masked versions and get predictions\n",
    "sample_x = sample_y.copy()\n",
    "for i in range(len(sample_x)):\n",
    "    n_mask = int(sample_x[i].size * 0.8)\n",
    "    flat_indices = np.random.choice(sample_x[i].size, size=n_mask, replace=False)\n",
    "    mask_indices = np.unravel_index(flat_indices, sample_x[i].shape)\n",
    "    sample_x[i][mask_indices] = 0\n",
    "\n",
    "# Get predictions\n",
    "predictions = model.predict(sample_x, verbose=0)\n",
    "\n",
    "# Feature names for labeling\n",
    "feature_names = [\n",
    "    'Weight on Bit',\n",
    "    'Rotary RPM',\n",
    "    'Total Pump Output',\n",
    "    'Rate Of Penetration',\n",
    "    'Standpipe Pressure',\n",
    "    'Rotary Torque',\n",
    "    'Hole Depth',\n",
    "    'Bit Depth'\n",
    "]\n",
    "\n",
    "# Create visualization for each example\n",
    "for example_idx in range(num_examples):\n",
    "    fig, axes = plt.subplots(8, 1, figsize=(16, 20))\n",
    "    fig.suptitle(f'Example {example_idx + 1}: Reconstruction Quality', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    original = sample_y[example_idx]\n",
    "    masked = sample_x[example_idx]\n",
    "    reconstructed = predictions[example_idx]\n",
    "    \n",
    "    # Plot each feature\n",
    "    for feature_idx in range(8):\n",
    "        ax = axes[feature_idx]\n",
    "        \n",
    "        # Original data (ground truth)\n",
    "        ax.plot(original[:, feature_idx], label='Original (Ground Truth)', \n",
    "                color='green', linewidth=2, alpha=0.8)\n",
    "        \n",
    "        # Masked input (what the model sees)\n",
    "        masked_feature = masked[:, feature_idx].copy()\n",
    "        masked_feature[masked_feature == 0] = np.nan  # Show gaps where masked\n",
    "        ax.plot(masked_feature, label='Masked Input', \n",
    "                color='red', linewidth=1.5, alpha=0.6, linestyle='--')\n",
    "        \n",
    "        # Reconstructed output\n",
    "        ax.plot(reconstructed[:, feature_idx], label='Reconstructed', \n",
    "                color='blue', linewidth=2, alpha=0.7)\n",
    "        \n",
    "        # Calculate error for this feature\n",
    "        feature_mae = mean_absolute_error(original[:, feature_idx], \n",
    "                                         reconstructed[:, feature_idx])\n",
    "        feature_rmse = np.sqrt(mean_squared_error(original[:, feature_idx], \n",
    "                                                  reconstructed[:, feature_idx]))\n",
    "        \n",
    "        ax.set_title(f'{feature_names[feature_idx]} - MAE: {feature_mae:.4f}, RMSE: {feature_rmse:.4f}', \n",
    "                    fontsize=10, fontweight='bold')\n",
    "        ax.set_xlabel('Time Step')\n",
    "        ax.set_ylabel('Normalized Value')\n",
    "        ax.legend(loc='upper right', fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim(-0.1, 1.1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print overall metrics for this example\n",
    "    example_mae = mean_absolute_error(original.reshape(-1), reconstructed.reshape(-1))\n",
    "    example_rmse = np.sqrt(mean_squared_error(original.reshape(-1), reconstructed.reshape(-1)))\n",
    "    print(f\"\\nExample {example_idx + 1} Overall Metrics:\")\n",
    "    print(f\"  MAE:  {example_mae:.6f}\")\n",
    "    print(f\"  RMSE: {example_rmse:.6f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a26344-16a6-4a05-9223-45f51f75cd22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e03cb0f8-1376-438a-87d6-0f0018524bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 23:23:29.090415: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-09 23:23:29.114989: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-09 23:23:29.341696: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-09 23:23:29.343122: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-09 23:23:29.886973: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import LSTM, RepeatVector, TimeDistributed, Dense\n",
    "from keras.models import Sequential\n",
    "from keras import Input\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "import collections\n",
    "from random import shuffle\n",
    "import itertools\n",
    "from os import listdir\n",
    "import random\n",
    "import string\n",
    "import statistics\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6033c331-d70b-4fe4-b69e-03603fa31c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.13.1\n",
      "NumPy: 1.23.5\n"
     ]
    }
   ],
   "source": [
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d9a31c1-ff4e-4132-a307-d78d2100c050",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'Weight on Bit (klbs)',\n",
    "    'Rotary RPM (RPM)',\n",
    "    'Total Pump Output (gal_per_min)',\n",
    "    'Rate Of Penetration (ft_per_hr)',\n",
    "    'Standpipe Pressure (psi)',\n",
    "    'Rotary Torque (kft_lb)', \n",
    "    'Hole Depth (feet)', \n",
    "    'Bit Depth (feet)'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a72daff5-bb2f-4f29-b5d9-f657e789d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_windows(dataset, columns):\n",
    "    df = pd.read_csv(os.path.join(\"Datasets\", \"MaskedAutoencoder\", dataset))\n",
    "    df = df[columns]\n",
    "\n",
    "    base_mask = (\n",
    "        (df[\"Hole Depth (feet)\"].rolling(10000).mean().diff() > 0) &\n",
    "        (df[\"Hole Depth (feet)\"] == df[\"Bit Depth (feet)\"]) &\n",
    "        (df[\"Hole Depth (feet)\"] > 1000)\n",
    "    )\n",
    "    \n",
    "    window = 100       # Rolling window size\n",
    "    threshold = 0.3    # Keep if rolling average > threshold\n",
    "    \n",
    "    # Compute rolling average of the mask (convert to 0/1 first)\n",
    "    rolling_avg = base_mask.astype(float).rolling(window).mean()\n",
    "    \n",
    "    # Final mask based on rolling average threshold\n",
    "    final_mask = (rolling_avg > threshold).fillna(0)\n",
    "    \n",
    "    final_mask = final_mask.astype(float).rolling(20000).mean() > 0.6\n",
    "    \n",
    "    masked_hole_depth = df[\"Hole Depth (feet)\"].where(final_mask, np.nan)\n",
    "    \n",
    "    gap_threshold = 100  # maximum number of consecutive NaNs to merge segments\n",
    "    \n",
    "    # Identify indices of non-NaN values\n",
    "    not_nan_idx = masked_hole_depth[masked_hole_depth.notna()].index\n",
    "    \n",
    "    # Grouping non-NaN indices based on closeness\n",
    "    groups = []\n",
    "    current_group = []\n",
    "    \n",
    "    for i, idx in enumerate(not_nan_idx):\n",
    "        if i == 0:\n",
    "            current_group.append(idx)\n",
    "            continue\n",
    "    \n",
    "        # Check gap from previous index\n",
    "        if idx - not_nan_idx[i-1] <= gap_threshold:\n",
    "            current_group.append(idx)\n",
    "        else:\n",
    "            groups.append(current_group)\n",
    "            current_group = [idx]\n",
    "    \n",
    "    # Append last group\n",
    "    if current_group:\n",
    "        groups.append(current_group)\n",
    "\n",
    "    # Fix all NaNs\n",
    "    drilling_segments = [  ]\n",
    "    window_size = 100\n",
    "    for group in groups:\n",
    "        dfg = df.loc[group].copy()\n",
    "        \n",
    "        for col in dfg.columns:\n",
    "            if np.issubdtype(dfg[col].dtype, np.number):\n",
    "                series = dfg[col]      \n",
    "                rolling_mean = series.rolling(window=window_size, min_periods=1, center=True).mean()\n",
    "                dfg[col] = series.fillna(rolling_mean).bfill(  ).ffill()\n",
    "    \n",
    "        drilling_segments.append(dfg)\n",
    "    \n",
    "    # Min Max Normalization\n",
    "    global_min = pd.concat(drilling_segments).min()\n",
    "    global_max = pd.concat(drilling_segments).max()\n",
    "    \n",
    "    # Step 2: Normalize each dataframe\n",
    "    print(f\"Drilling Segments: {len(drilling_segments)}\")\n",
    "    normalized_drilling_segments = []\n",
    "    for df in drilling_segments:\n",
    "        normalized_df = (df - global_min) / (global_max - global_min)\n",
    "        normalized_drilling_segments.append(normalized_df)\n",
    "\n",
    "    window_size = 60 * 10  # 10 minutes\n",
    "\n",
    "    windows = []\n",
    "    count = 1\n",
    "    for df in normalized_drilling_segments:\n",
    "        print(f\"\\t{count}\")\n",
    "        count += 1\n",
    "        for i in range(len(df) - window_size + 1):\n",
    "            window = df.iloc[i:i + window_size]\n",
    "            windows.append(window.to_numpy())\n",
    "\n",
    "    print(f\"Windows: {len(windows):,}\".replace(',', ' ')) \n",
    "    print(f\"Windows per Segment: {len(windows) / len(drilling_segments):,.2f}\".replace(',', ' '))\n",
    "    \n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81ed19b3-8fee-4b2a-bf9d-93925474f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_data(data, MASKING_PERCENT=0.8):\n",
    "    masked_data = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"Processing: {i}/{len(data)} ({100*i/len(data):.1f}%)\")\n",
    "    \n",
    "        arr = np.array(data[i], dtype=np.float32)  # Use float32 instead of float64\n",
    "        masked_arr = arr.copy()\n",
    "        \n",
    "        total_elements = arr.size\n",
    "        n_mask = int(total_elements * MASKING_PERCENT)\n",
    "        \n",
    "        flat_indices = np.random.choice(total_elements, size=n_mask, replace=False)\n",
    "        mask_indices = np.unravel_index(flat_indices, arr.shape)\n",
    "        \n",
    "        masked_arr[mask_indices] = 0  # Use 0 instead of NaN for neural networks\n",
    "        \n",
    "        masked_data.append(masked_arr)\n",
    "    \n",
    "    return np.array(masked_data, dtype=np.float32)  # Return as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0bff3db-3c83-4e70-9d34-93d94d93bd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data, batch_size=32, mask_percent=0.8, shuffle=True):\n",
    "        self.data = np.array(data, dtype=np.float32)\n",
    "        self.batch_size = batch_size\n",
    "        self.mask_percent = mask_percent\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(data))\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.data[batch_indices]\n",
    "        \n",
    "        # Create masked version (X) from original (Y)\n",
    "        batch_x = batch_y.copy()\n",
    "        for i in range(len(batch_x)):\n",
    "            n_mask = int(batch_x[i].size * self.mask_percent)\n",
    "            flat_indices = np.random.choice(batch_x[i].size, size=n_mask, replace=False)\n",
    "            mask_indices = np.unravel_index(flat_indices, batch_x[i].shape)\n",
    "            batch_x[i][mask_indices] = 0\n",
    "            \n",
    "        return batch_x, batch_y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6182e07c-0156-4180-b2c3-70dbbc8ca326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder training: 78B-32 1 sec data 27200701.csv, 27029986-3.csv\n",
    "# Task Header 1 (DAS Stickslip): 27029986-4.csv\n",
    "# Task Header 2 (Temp OUT (Degrees)): 27029986-5.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b7142e-8ece-47a0-95ab-311c27f332c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drilling Segments: 3\n",
      "\t1\n",
      "\t2\n",
      "\t3\n",
      "Windows: 131 072\n",
      "Windows per Segment: 43 690.67\n",
      "Drilling Segments: 9\n",
      "\t1\n",
      "\t2\n",
      "\t3\n",
      "\t4\n",
      "\t5\n",
      "\t6\n",
      "\t7\n"
     ]
    }
   ],
   "source": [
    "windows1 = csv_to_windows(\"27029986-3.csv\", columns)\n",
    "windows2 = csv_to_windows(\"78B-32 1 sec data 27200701.csv\", columns)\n",
    "\n",
    "# Shuffle both lists\n",
    "random.seed(42)\n",
    "random.shuffle(windows1)\n",
    "random.shuffle(windows2)\n",
    "\n",
    "# Take the same amount from each (the minimum length)\n",
    "min_length = min(len(windows1), len(windows2))\n",
    "windows1_sampled = windows1[:min_length]\n",
    "windows2_sampled = windows2[:min_length]\n",
    "\n",
    "# Combine them\n",
    "windows = windows1_sampled + windows2_sampled\n",
    "\n",
    "# Shuffle the combined list\n",
    "random.shuffle(windows)\n",
    "\n",
    "print(f\"Sampled {min_length:,} from each list\".replace(',', ' '))\n",
    "print(f\"Total windows: {len(windows):,}\".replace(',', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3334675-ea9d-4d11-a6ea-895666143705",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSET_PERCENT = 0.3  # Use 30% of windows (adjust as needed)\n",
    "\n",
    "# Calculate how many windows to keep\n",
    "n_windows_to_keep = int(len(windows) * SUBSET_PERCENT)\n",
    "\n",
    "# Randomly sample indices\n",
    "random.seed(42)\n",
    "subset_indices = random.sample(range(len(windows)), n_windows_to_keep)\n",
    "\n",
    "# Select the subset\n",
    "windows = [windows[i] for i in subset_indices]\n",
    "\n",
    "print(f\"ðŸ“Š Subset Selection:\")\n",
    "print(f\"   Original windows: {len(windows) / SUBSET_PERCENT:,.0f}\".replace(',', ' '))\n",
    "print(f\"   Subset percent: {SUBSET_PERCENT * 100:.1f}%\")\n",
    "print(f\"   Selected windows: {len(windows):,}\".replace(',', ' '))\n",
    "print(f\"   Estimated memory reduction: {(1 - SUBSET_PERCENT) * 100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fa2d00-47e3-46a3-ad90-b3ee040123dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 - Split into train/test and convert to numpy ONCE\n",
    "train_windows, test_windows = train_test_split(windows, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to numpy arrays (only store Y, not X)\n",
    "train_windows_y = np.array(train_windows, dtype=np.float32)\n",
    "test_windows_y = np.array(test_windows, dtype=np.float32)\n",
    "\n",
    "# Free up memory\n",
    "del windows, windows1, windows2, windows1_sampled, windows2_sampled, train_windows, test_windows\n",
    "\n",
    "print(f\"Train shape: {train_windows_y.shape}\")\n",
    "print(f\"Test shape: {test_windows_y.shape}\")\n",
    "print(f\"Memory for train_y: {train_windows_y.nbytes / 1e9:.2f} GB\")\n",
    "print(f\"Memory for test_y: {test_windows_y.nbytes / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5ede37-182a-43a4-8340-982433b5af2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 - Create generators (NO masking done here, just setup)\n",
    "train_gen = MaskedDataGenerator(train_windows_y, batch_size=32, mask_percent=0.8, shuffle=True)\n",
    "test_gen = MaskedDataGenerator(test_windows_y, batch_size=32, mask_percent=0.8, shuffle=False)\n",
    "\n",
    "print(f\"Train batches per epoch: {len(train_gen)}\")\n",
    "print(f\"Test batches: {len(test_gen)}\")\n",
    "\n",
    "# Test that it works\n",
    "batch_x, batch_y = train_gen[0]\n",
    "print(f\"Batch X shape: {batch_x.shape}\")\n",
    "print(f\"Batch Y shape: {batch_y.shape}\")\n",
    "print(f\"Masking working: {np.sum(batch_x == 0) > 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45051a5-5e72-48c1-ac64-9e024e067f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed MetricsCallback - Memory efficient without K.clear_session()\n",
    "\n",
    "class MetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, train_gen, test_gen, subset_fraction=0.05):\n",
    "        \"\"\"\n",
    "        Memory-efficient metrics callback\n",
    "        \n",
    "        Args:\n",
    "            train_gen: Training data generator\n",
    "            test_gen: Test data generator\n",
    "            subset_fraction: Fraction of batches to evaluate (default: 0.05 = 5%)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.train_gen = train_gen\n",
    "        self.test_gen = test_gen\n",
    "        self.subset_fraction = subset_fraction\n",
    "        self.history = {\n",
    "            'train_rmse': [],\n",
    "            'train_mae': [],\n",
    "            'test_rmse': [],\n",
    "            'test_mae': []\n",
    "        }\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        import gc\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Calculating metrics for epoch {epoch + 1}...\")\n",
    "        print(f\"   (Evaluating on {self.subset_fraction*100:.1f}% of batches)\")\n",
    "        \n",
    "        # Calculate number of batches to evaluate\n",
    "        max_train_batches = max(1, int(len(self.train_gen) * self.subset_fraction))\n",
    "        max_test_batches = max(1, int(len(self.test_gen) * self.subset_fraction))\n",
    "        \n",
    "        # === TRAINING METRICS ===\n",
    "        train_mse_list = []\n",
    "        train_mae_list = []\n",
    "        \n",
    "        print(f\"   Training: 0/{max_train_batches}\", end=\"\", flush=True)\n",
    "        \n",
    "        for i in range(max_train_batches):\n",
    "            # Get batch\n",
    "            batch_x, batch_y = self.train_gen[i]\n",
    "            \n",
    "            # Predict (use predict_on_batch for efficiency)\n",
    "            batch_pred = self.model.predict_on_batch(batch_x)\n",
    "            \n",
    "            # Calculate metrics and store\n",
    "            train_mse_list.append(np.mean((batch_y - batch_pred) ** 2))\n",
    "            train_mae_list.append(np.mean(np.abs(batch_y - batch_pred)))\n",
    "            \n",
    "            # Immediately delete to free memory\n",
    "            del batch_x, batch_y, batch_pred\n",
    "            \n",
    "            # Progress update\n",
    "            if (i + 1) % 50 == 0 or (i + 1) == max_train_batches:\n",
    "                print(f\"\\r   Training: {i+1}/{max_train_batches}\", end=\"\", flush=True)\n",
    "                gc.collect()\n",
    "        \n",
    "        # Calculate final metrics\n",
    "        train_rmse = np.sqrt(np.mean(train_mse_list))\n",
    "        train_mae = np.mean(train_mae_list)\n",
    "        print(f\"\\r   Training: {max_train_batches}/{max_train_batches} - RMSE: {train_rmse:.6f}, MAE: {train_mae:.6f}\")\n",
    "        \n",
    "        # Clear lists\n",
    "        del train_mse_list, train_mae_list\n",
    "        gc.collect()\n",
    "        \n",
    "        # === TEST METRICS ===\n",
    "        test_mse_list = []\n",
    "        test_mae_list = []\n",
    "        \n",
    "        print(f\"   Testing: 0/{max_test_batches}\", end=\"\", flush=True)\n",
    "        \n",
    "        for i in range(max_test_batches):\n",
    "            # Get batch\n",
    "            batch_x, batch_y = self.test_gen[i]\n",
    "            \n",
    "            # Predict\n",
    "            batch_pred = self.model.predict_on_batch(batch_x)\n",
    "            \n",
    "            # Calculate metrics and store\n",
    "            test_mse_list.append(np.mean((batch_y - batch_pred) ** 2))\n",
    "            test_mae_list.append(np.mean(np.abs(batch_y - batch_pred)))\n",
    "            \n",
    "            # Immediately delete\n",
    "            del batch_x, batch_y, batch_pred\n",
    "            \n",
    "            # Progress update\n",
    "            if (i + 1) % 25 == 0 or (i + 1) == max_test_batches:\n",
    "                print(f\"\\r   Testing: {i+1}/{max_test_batches}\", end=\"\", flush=True)\n",
    "                gc.collect()\n",
    "        \n",
    "        # Calculate final metrics\n",
    "        test_rmse = np.sqrt(np.mean(test_mse_list))\n",
    "        test_mae = np.mean(test_mae_list)\n",
    "        print(f\"\\r   Testing: {max_test_batches}/{max_test_batches} - RMSE: {test_rmse:.6f}, MAE: {test_mae:.6f}\")\n",
    "        \n",
    "        # Clear lists\n",
    "        del test_mse_list, test_mae_list\n",
    "        gc.collect()\n",
    "        \n",
    "        # Store metrics as Python floats (not numpy types)\n",
    "        self.history['train_rmse'].append(float(train_rmse))\n",
    "        self.history['train_mae'].append(float(train_mae))\n",
    "        self.history['test_rmse'].append(float(test_rmse))\n",
    "        self.history['test_mae'].append(float(test_mae))\n",
    "        \n",
    "        print(f\"   âœ… Epoch {epoch + 1} complete\")\n",
    "\n",
    "\n",
    "# Create the callback with 5% evaluation (reduced from 10%)\n",
    "metrics_callback = MetricsCallback(\n",
    "    train_gen, \n",
    "    test_gen,\n",
    "    subset_fraction=0.05  # Only evaluate 5% of batches\n",
    ")\n",
    "\n",
    "print(\"âœ… Memory-efficient callback created!\")\n",
    "print(f\"   Will evaluate ~{int(len(train_gen)*0.05)} training batches and ~{int(len(test_gen)*0.05)} test batches per epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8596d76f-c197-4255-9f29-96c0b21d74e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 - Build model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, activation='tanh', input_shape=(600, 8), return_sequences=True))\n",
    "model.add(LSTM(64, activation='tanh', return_sequences=False))\n",
    "model.add(RepeatVector(600))\n",
    "model.add(LSTM(64, activation='tanh', return_sequences=True))\n",
    "model.add(LSTM(128, activation='tanh', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(8)))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1074ecb-8334-482d-acb5-2752242a0b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11 - Train model with the callback\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    epochs=10,\n",
    "    validation_data=test_gen,\n",
    "    callbacks=[metrics_callback],  # Add the callback here\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40652bb9-2646-45e5-abcd-866bcf255984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['loss'], label='Training loss', marker='o')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss', marker='o')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training History - Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot RMSE\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(metrics_callback.history['train_rmse'], label='Train RMSE', marker='o')\n",
    "plt.plot(metrics_callback.history['test_rmse'], label='Test RMSE', marker='o')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Training History - RMSE')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot MAE\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(metrics_callback.history['train_mae'], label='Train MAE', marker='o')\n",
    "plt.plot(metrics_callback.history['test_mae'], label='Test MAE', marker='o')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('Training History - MAE')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0701a0-7455-486f-b4c4-ea54bf4d4d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12 - Display final metrics table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"METRICS PER EPOCH\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Epoch':<8} {'Train RMSE':<15} {'Train MAE':<15} {'Test RMSE':<15} {'Test MAE':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i in range(len(metrics_callback.history['train_rmse'])):\n",
    "    print(f\"{i+1:<8} \"\n",
    "          f\"{metrics_callback.history['train_rmse'][i]:<15.6f} \"\n",
    "          f\"{metrics_callback.history['train_mae'][i]:<15.6f} \"\n",
    "          f\"{metrics_callback.history['test_rmse'][i]:<15.6f} \"\n",
    "          f\"{metrics_callback.history['test_mae'][i]:<15.6f}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Final epoch summary\n",
    "final_epoch = len(metrics_callback.history['train_rmse']) - 1\n",
    "print(f\"\\nðŸ“Š Final Epoch ({final_epoch + 1}) Summary:\")\n",
    "print(f\"   Training   - RMSE: {metrics_callback.history['train_rmse'][final_epoch]:.6f}, \"\n",
    "      f\"MAE: {metrics_callback.history['train_mae'][final_epoch]:.6f}\")\n",
    "print(f\"   Test       - RMSE: {metrics_callback.history['test_rmse'][final_epoch]:.6f}, \"\n",
    "      f\"MAE: {metrics_callback.history['test_mae'][final_epoch]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e1b192-9765-4130-9837-646e079bc69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a few random test samples\n",
    "np.random.seed(42)\n",
    "num_examples = 3\n",
    "random_indices = np.random.choice(len(test_windows_y), num_examples, replace=False)\n",
    "\n",
    "# Get the samples\n",
    "sample_y = test_windows_y[random_indices]\n",
    "\n",
    "# Create masked versions and get predictions\n",
    "sample_x = sample_y.copy()\n",
    "for i in range(len(sample_x)):\n",
    "    n_mask = int(sample_x[i].size * 0.8)\n",
    "    flat_indices = np.random.choice(sample_x[i].size, size=n_mask, replace=False)\n",
    "    mask_indices = np.unravel_index(flat_indices, sample_x[i].shape)\n",
    "    sample_x[i][mask_indices] = 0\n",
    "\n",
    "# Get predictions\n",
    "predictions = model.predict(sample_x, verbose=0)\n",
    "\n",
    "# Feature names for labeling\n",
    "feature_names = [\n",
    "    'Weight on Bit',\n",
    "    'Rotary RPM',\n",
    "    'Total Pump Output',\n",
    "    'Rate Of Penetration',\n",
    "    'Standpipe Pressure',\n",
    "    'Rotary Torque',\n",
    "    'Hole Depth',\n",
    "    'Bit Depth'\n",
    "]\n",
    "\n",
    "# Create visualization for each example\n",
    "for example_idx in range(num_examples):\n",
    "    fig, axes = plt.subplots(8, 1, figsize=(16, 20))\n",
    "    fig.suptitle(f'Example {example_idx + 1}: Reconstruction Quality', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    original = sample_y[example_idx]\n",
    "    masked = sample_x[example_idx]\n",
    "    reconstructed = predictions[example_idx]\n",
    "    \n",
    "    # Plot each feature\n",
    "    for feature_idx in range(8):\n",
    "        ax = axes[feature_idx]\n",
    "        \n",
    "        # Original data (ground truth)\n",
    "        ax.plot(original[:, feature_idx], label='Original (Ground Truth)', \n",
    "                color='green', linewidth=2, alpha=0.8)\n",
    "        \n",
    "        # Masked input (what the model sees)\n",
    "        masked_feature = masked[:, feature_idx].copy()\n",
    "        masked_feature[masked_feature == 0] = np.nan  # Show gaps where masked\n",
    "        ax.plot(masked_feature, label='Masked Input', \n",
    "                color='red', linewidth=1.5, alpha=0.6, linestyle='--')\n",
    "        \n",
    "        # Reconstructed output\n",
    "        ax.plot(reconstructed[:, feature_idx], label='Reconstructed', \n",
    "                color='blue', linewidth=2, alpha=0.7)\n",
    "        \n",
    "        # Calculate error for this feature\n",
    "        feature_mae = mean_absolute_error(original[:, feature_idx], \n",
    "                                         reconstructed[:, feature_idx])\n",
    "        feature_rmse = np.sqrt(mean_squared_error(original[:, feature_idx], \n",
    "                                                  reconstructed[:, feature_idx]))\n",
    "        \n",
    "        ax.set_title(f'{feature_names[feature_idx]} - MAE: {feature_mae:.4f}, RMSE: {feature_rmse:.4f}', \n",
    "                    fontsize=10, fontweight='bold')\n",
    "        ax.set_xlabel('Time Step')\n",
    "        ax.set_ylabel('Normalized Value')\n",
    "        ax.legend(loc='upper right', fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim(-0.1, 1.1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print overall metrics for this example\n",
    "    example_mae = mean_absolute_error(original.reshape(-1), reconstructed.reshape(-1))\n",
    "    example_rmse = np.sqrt(mean_squared_error(original.reshape(-1), reconstructed.reshape(-1)))\n",
    "    print(f\"\\nExample {example_idx + 1} Overall Metrics:\")\n",
    "    print(f\"  MAE:  {example_mae:.6f}\")\n",
    "    print(f\"  RMSE: {example_rmse:.6f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a26344-16a6-4a05-9223-45f51f75cd22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

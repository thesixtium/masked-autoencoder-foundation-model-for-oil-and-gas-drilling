{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e03cb0f8-1376-438a-87d6-0f0018524bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 22:51:21.062013: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-08 22:51:21.063305: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-08 22:51:21.086198: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-08 22:51:21.086678: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-08 22:51:21.436135: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import LSTM, RepeatVector, TimeDistributed, Dense\n",
    "from keras.models import Sequential\n",
    "from keras import Input\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "import collections\n",
    "from random import shuffle\n",
    "import itertools\n",
    "from os import listdir\n",
    "import random\n",
    "import string\n",
    "import statistics\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6033c331-d70b-4fe4-b69e-03603fa31c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.13.1\n",
      "NumPy: 1.23.5\n"
     ]
    }
   ],
   "source": [
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d9a31c1-ff4e-4132-a307-d78d2100c050",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'Weight on Bit (klbs)',\n",
    "    'Rotary RPM (RPM)',\n",
    "    'Total Pump Output (gal_per_min)',\n",
    "    'Rate Of Penetration (ft_per_hr)',\n",
    "    'Standpipe Pressure (psi)',\n",
    "    'Rotary Torque (kft_lb)', \n",
    "    'Hole Depth (feet)', \n",
    "    'Bit Depth (feet)'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a72daff5-bb2f-4f29-b5d9-f657e789d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_windows(dataset, columns):\n",
    "    df = pd.read_csv(os.path.join(\"Datasets\", \"MaskedAutoencoder\", dataset))\n",
    "    df = df[columns]\n",
    "\n",
    "    base_mask = (\n",
    "        (df[\"Hole Depth (feet)\"].rolling(10000).mean().diff() > 0) &\n",
    "        (df[\"Hole Depth (feet)\"] == df[\"Bit Depth (feet)\"]) &\n",
    "        (df[\"Hole Depth (feet)\"] > 1000)\n",
    "    )\n",
    "    \n",
    "    window = 100       # Rolling window size\n",
    "    threshold = 0.3    # Keep if rolling average > threshold\n",
    "    \n",
    "    # Compute rolling average of the mask (convert to 0/1 first)\n",
    "    rolling_avg = base_mask.astype(float).rolling(window).mean()\n",
    "    \n",
    "    # Final mask based on rolling average threshold\n",
    "    final_mask = (rolling_avg > threshold).fillna(0)\n",
    "    \n",
    "    final_mask = final_mask.astype(float).rolling(20000).mean() > 0.6\n",
    "    \n",
    "    masked_hole_depth = df[\"Hole Depth (feet)\"].where(final_mask, np.nan)\n",
    "    \n",
    "    gap_threshold = 100  # maximum number of consecutive NaNs to merge segments\n",
    "    \n",
    "    # Identify indices of non-NaN values\n",
    "    not_nan_idx = masked_hole_depth[masked_hole_depth.notna()].index\n",
    "    \n",
    "    # Grouping non-NaN indices based on closeness\n",
    "    groups = []\n",
    "    current_group = []\n",
    "    \n",
    "    for i, idx in enumerate(not_nan_idx):\n",
    "        if i == 0:\n",
    "            current_group.append(idx)\n",
    "            continue\n",
    "    \n",
    "        # Check gap from previous index\n",
    "        if idx - not_nan_idx[i-1] <= gap_threshold:\n",
    "            current_group.append(idx)\n",
    "        else:\n",
    "            groups.append(current_group)\n",
    "            current_group = [idx]\n",
    "    \n",
    "    # Append last group\n",
    "    if current_group:\n",
    "        groups.append(current_group)\n",
    "\n",
    "    # Fix all NaNs\n",
    "    drilling_segments = [  ]\n",
    "    window_size = 100\n",
    "    for group in groups:\n",
    "        dfg = df.loc[group].copy()\n",
    "        \n",
    "        for col in dfg.columns:\n",
    "            if np.issubdtype(dfg[col].dtype, np.number):\n",
    "                series = dfg[col]      \n",
    "                rolling_mean = series.rolling(window=window_size, min_periods=1, center=True).mean()\n",
    "                dfg[col] = series.fillna(rolling_mean).bfill(  ).ffill()\n",
    "    \n",
    "        drilling_segments.append(dfg)\n",
    "    \n",
    "    # Min Max Normalization\n",
    "    global_min = pd.concat(drilling_segments).min()\n",
    "    global_max = pd.concat(drilling_segments).max()\n",
    "    \n",
    "    # Step 2: Normalize each dataframe\n",
    "    print(f\"Drilling Segments: {len(drilling_segments)}\")\n",
    "    normalized_drilling_segments = []\n",
    "    for df in drilling_segments:\n",
    "        normalized_df = (df - global_min) / (global_max - global_min)\n",
    "        normalized_drilling_segments.append(normalized_df)\n",
    "\n",
    "    window_size = 60 * 10  # 10 minutes\n",
    "\n",
    "    windows = []\n",
    "    count = 1\n",
    "    for df in normalized_drilling_segments:\n",
    "        print(f\"\\t{count}\")\n",
    "        count += 1\n",
    "        for i in range(len(df) - window_size + 1):\n",
    "            window = df.iloc[i:i + window_size]\n",
    "            windows.append(window.to_numpy())\n",
    "\n",
    "    print(f\"Windows: {len(windows):,}\".replace(',', ' ')) \n",
    "    print(f\"Windows per Segment: {len(windows) / len(drilling_segments):,.2f}\".replace(',', ' '))\n",
    "    \n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81ed19b3-8fee-4b2a-bf9d-93925474f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_data(data, MASKING_PERCENT=0.8):\n",
    "    masked_data = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"Processing: {i}/{len(data)} ({100*i/len(data):.1f}%)\")\n",
    "    \n",
    "        arr = np.array(data[i], dtype=np.float32)  # Use float32 instead of float64\n",
    "        masked_arr = arr.copy()\n",
    "        \n",
    "        total_elements = arr.size\n",
    "        n_mask = int(total_elements * MASKING_PERCENT)\n",
    "        \n",
    "        flat_indices = np.random.choice(total_elements, size=n_mask, replace=False)\n",
    "        mask_indices = np.unravel_index(flat_indices, arr.shape)\n",
    "        \n",
    "        masked_arr[mask_indices] = 0  # Use 0 instead of NaN for neural networks\n",
    "        \n",
    "        masked_data.append(masked_arr)\n",
    "    \n",
    "    return np.array(masked_data, dtype=np.float32)  # Return as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0bff3db-3c83-4e70-9d34-93d94d93bd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data, batch_size=32, mask_percent=0.8, shuffle=True):\n",
    "        self.data = np.array(data, dtype=np.float32)\n",
    "        self.batch_size = batch_size\n",
    "        self.mask_percent = mask_percent\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(data))\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.data[batch_indices]\n",
    "        \n",
    "        # Create masked version (X) from original (Y)\n",
    "        batch_x = batch_y.copy()\n",
    "        for i in range(len(batch_x)):\n",
    "            n_mask = int(batch_x[i].size * self.mask_percent)\n",
    "            flat_indices = np.random.choice(batch_x[i].size, size=n_mask, replace=False)\n",
    "            mask_indices = np.unravel_index(flat_indices, batch_x[i].shape)\n",
    "            batch_x[i][mask_indices] = 0\n",
    "            \n",
    "        return batch_x, batch_y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6182e07c-0156-4180-b2c3-70dbbc8ca326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder training: 78B-32 1 sec data 27200701.csv, 27029986-3.csv\n",
    "# Task Header 1 (DAS Stickslip): 27029986-4.csv\n",
    "# Task Header 2 (Temp OUT (Degrees)): 27029986-5.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8b7142e-8ece-47a0-95ab-311c27f332c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drilling Segments: 3\n",
      "\t1\n",
      "\t2\n",
      "\t3\n",
      "Windows: 131 072\n",
      "Windows per Segment: 43 690.67\n",
      "Drilling Segments: 9\n",
      "\t1\n",
      "\t2\n",
      "\t3\n",
      "\t4\n",
      "\t5\n",
      "\t6\n",
      "\t7\n",
      "\t8\n",
      "\t9\n",
      "Windows: 332 561\n",
      "Windows per Segment: 36 951.22\n",
      "Sampled 131 072 from each list\n",
      "Total windows: 262 144\n"
     ]
    }
   ],
   "source": [
    "windows1 = csv_to_windows(\"27029986-3.csv\", columns)\n",
    "windows2 = csv_to_windows(\"78B-32 1 sec data 27200701.csv\", columns)\n",
    "\n",
    "# Shuffle both lists\n",
    "random.seed(42)\n",
    "random.shuffle(windows1)\n",
    "random.shuffle(windows2)\n",
    "\n",
    "# Take the same amount from each (the minimum length)\n",
    "min_length = min(len(windows1), len(windows2))\n",
    "windows1_sampled = windows1[:min_length]\n",
    "windows2_sampled = windows2[:min_length]\n",
    "\n",
    "# Combine them\n",
    "windows = windows1_sampled + windows2_sampled\n",
    "\n",
    "# Shuffle the combined list\n",
    "random.shuffle(windows)\n",
    "\n",
    "print(f\"Sampled {min_length:,} from each list\".replace(',', ' '))\n",
    "print(f\"Total windows: {len(windows):,}\".replace(',', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77fa2d00-47e3-46a3-ad90-b3ee040123dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (209715, 600, 8)\n",
      "Test shape: (52429, 600, 8)\n",
      "Memory for train_y: 4.03 GB\n",
      "Memory for test_y: 1.01 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 - Split into train/test and convert to numpy ONCE\n",
    "train_windows, test_windows = train_test_split(windows, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to numpy arrays (only store Y, not X)\n",
    "train_windows_y = np.array(train_windows, dtype=np.float32)\n",
    "test_windows_y = np.array(test_windows, dtype=np.float32)\n",
    "\n",
    "# Free up memory\n",
    "del windows, windows1, windows2, windows1_sampled, windows2_sampled, train_windows, test_windows\n",
    "\n",
    "print(f\"Train shape: {train_windows_y.shape}\")\n",
    "print(f\"Test shape: {test_windows_y.shape}\")\n",
    "print(f\"Memory for train_y: {train_windows_y.nbytes / 1e9:.2f} GB\")\n",
    "print(f\"Memory for test_y: {test_windows_y.nbytes / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce5ede37-182a-43a4-8340-982433b5af2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches per epoch: 6554\n",
      "Test batches: 1639\n",
      "Batch X shape: (32, 600, 8)\n",
      "Batch Y shape: (32, 600, 8)\n",
      "Masking working: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 - Create generators (NO masking done here, just setup)\n",
    "train_gen = MaskedDataGenerator(train_windows_y, batch_size=32, mask_percent=0.8, shuffle=True)\n",
    "test_gen = MaskedDataGenerator(test_windows_y, batch_size=32, mask_percent=0.8, shuffle=False)\n",
    "\n",
    "print(f\"Train batches per epoch: {len(train_gen)}\")\n",
    "print(f\"Test batches: {len(test_gen)}\")\n",
    "\n",
    "# Test that it works\n",
    "batch_x, batch_y = train_gen[0]\n",
    "print(f\"Batch X shape: {batch_x.shape}\")\n",
    "print(f\"Batch Y shape: {batch_y.shape}\")\n",
    "print(f\"Masking working: {np.sum(batch_x == 0) > 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c45051a5-5e72-48c1-ac64-9e024e067f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, train_gen, test_gen, train_y, test_y):\n",
    "        super().__init__()\n",
    "        self.train_gen = train_gen\n",
    "        self.test_gen = test_gen\n",
    "        self.train_y = train_y\n",
    "        self.test_y = test_y\n",
    "        self.history = {\n",
    "            'train_rmse': [],\n",
    "            'train_mae': [],\n",
    "            'test_rmse': [],\n",
    "            'test_mae': []\n",
    "        }\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"\\nðŸ“Š Calculating metrics for epoch {epoch + 1}...\")\n",
    "        \n",
    "        # Training metrics\n",
    "        train_pred = self.model.predict(self.train_gen, verbose=0)\n",
    "        train_rmse = np.sqrt(mean_squared_error(\n",
    "            self.train_y.reshape(-1), \n",
    "            train_pred.reshape(-1)\n",
    "        ))\n",
    "        train_mae = mean_absolute_error(\n",
    "            self.train_y.reshape(-1), \n",
    "            train_pred.reshape(-1)\n",
    "        )\n",
    "        \n",
    "        # Test metrics\n",
    "        test_pred = self.model.predict(self.test_gen, verbose=0)\n",
    "        test_rmse = np.sqrt(mean_squared_error(\n",
    "            self.test_y.reshape(-1), \n",
    "            test_pred.reshape(-1)\n",
    "        ))\n",
    "        test_mae = mean_absolute_error(\n",
    "            self.test_y.reshape(-1), \n",
    "            test_pred.reshape(-1)\n",
    "        )\n",
    "        \n",
    "        # Store metrics\n",
    "        self.history['train_rmse'].append(train_rmse)\n",
    "        self.history['train_mae'].append(train_mae)\n",
    "        self.history['test_rmse'].append(test_rmse)\n",
    "        self.history['test_mae'].append(test_mae)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"   Train - RMSE: {train_rmse:.6f}, MAE: {train_mae:.6f}\")\n",
    "        print(f\"   Test  - RMSE: {test_rmse:.6f}, MAE: {test_mae:.6f}\")\n",
    "\n",
    "# Create the callback\n",
    "metrics_callback = MetricsCallback(train_gen, test_gen, train_windows_y, test_windows_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8596d76f-c197-4255-9f29-96c0b21d74e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 22:51:53.685641: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 600, 128)          70144     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " repeat_vector (RepeatVecto  (None, 600, 64)           0         \n",
      " r)                                                              \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 600, 64)           33024     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 600, 128)          98816     \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 600, 8)            1032      \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 252424 (986.03 KB)\n",
      "Trainable params: 252424 (986.03 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Cell 10 - Build model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, activation='tanh', input_shape=(600, 8), return_sequences=True))\n",
    "model.add(LSTM(64, activation='tanh', return_sequences=False))\n",
    "model.add(RepeatVector(600))\n",
    "model.add(LSTM(64, activation='tanh', return_sequences=True))\n",
    "model.add(LSTM(128, activation='tanh', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(8)))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1074ecb-8334-482d-acb5-2752242a0b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "  27/6554 [..............................] - ETA: 37:33 - loss: 0.1131"
     ]
    }
   ],
   "source": [
    "# Cell 11 - Train model with the callback\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    epochs=3,\n",
    "    validation_data=test_gen,\n",
    "    callbacks=[metrics_callback],  # Add the callback here\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40652bb9-2646-45e5-abcd-866bcf255984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['loss'], label='Training loss', marker='o')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss', marker='o')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training History - Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot RMSE\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(metrics_callback.history['train_rmse'], label='Train RMSE', marker='o')\n",
    "plt.plot(metrics_callback.history['test_rmse'], label='Test RMSE', marker='o')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Training History - RMSE')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot MAE\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(metrics_callback.history['train_mae'], label='Train MAE', marker='o')\n",
    "plt.plot(metrics_callback.history['test_mae'], label='Test MAE', marker='o')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('Training History - MAE')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0701a0-7455-486f-b4c4-ea54bf4d4d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12 - Display final metrics table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"METRICS PER EPOCH\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Epoch':<8} {'Train RMSE':<15} {'Train MAE':<15} {'Test RMSE':<15} {'Test MAE':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i in range(len(metrics_callback.history['train_rmse'])):\n",
    "    print(f\"{i+1:<8} \"\n",
    "          f\"{metrics_callback.history['train_rmse'][i]:<15.6f} \"\n",
    "          f\"{metrics_callback.history['train_mae'][i]:<15.6f} \"\n",
    "          f\"{metrics_callback.history['test_rmse'][i]:<15.6f} \"\n",
    "          f\"{metrics_callback.history['test_mae'][i]:<15.6f}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Final epoch summary\n",
    "final_epoch = len(metrics_callback.history['train_rmse']) - 1\n",
    "print(f\"\\nðŸ“Š Final Epoch ({final_epoch + 1}) Summary:\")\n",
    "print(f\"   Training   - RMSE: {metrics_callback.history['train_rmse'][final_epoch]:.6f}, \"\n",
    "      f\"MAE: {metrics_callback.history['train_mae'][final_epoch]:.6f}\")\n",
    "print(f\"   Test       - RMSE: {metrics_callback.history['test_rmse'][final_epoch]:.6f}, \"\n",
    "      f\"MAE: {metrics_callback.history['test_mae'][final_epoch]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e1b192-9765-4130-9837-646e079bc69e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
